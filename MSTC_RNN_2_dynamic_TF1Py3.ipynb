{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTINUATION TO:\n",
    "## <font color='brown'>Recurrent Neural Networks in Tensorflow I </font>\n",
    "\n",
    "### from:\n",
    "### http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The differences with MSTC_RNN_1 are:\n",
    "\n",
    "* <font color='red' size='3'>**Translating our model to Tensorflow **</font>\n",
    "  \n",
    "* <font color='red' size='3'>**Using dynamic RNN **</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>First: dealing with data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Generate our binary sequences**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Prepare for feeding data into the graph:** *from raw data to batches*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adapted from \n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Second: we will use a rnn_cell in TENSORFLOW's API *tf.nn.rnn_cell.BasicRNNCell*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Set GLOBAL Configuration Variables**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "num_epochs=1\n",
    "num_steps = 5 # number of truncated backprop steps\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 8\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Defining the graph model**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* <font size='4'>**Using a dymanic RNN requires rnn_inputs = **</font>\n",
    "<font color='red' size='4'>  A 3-dimnesional tensor of shape [batch_size, num_steps, features]</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dynamic RNN : rnn_inputs = Tensor batch_size x num_steps x no_inputs\n",
    "rnn_inputs = tf.one_hot(x, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Translating our model to a BasicRNNCel in Tensorflowâ€™s API is easy:</font>\n",
    "* **We simply replace two sections by two lines!!!**\n",
    "* **We use:** <font color='red' size='3'>*tf.nn.dynamic_rnn*</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Definition of rnn_cell in TENSORFLOW's API\n",
    "\"\"\"\n",
    "\n",
    "# For TF 1.\n",
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "#cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "#cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "#rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "#\n",
    "#THAT WAS PREVIOUS Definition of rnn_cell without API\n",
    "\n",
    "#with tf.variable_scope('rnn_cell'):\n",
    "#    W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "#    b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "#def rnn_cell(rnn_input, state):\n",
    "#    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "#        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "#        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "#    return tf.tanh(tf.matmul(tf.concat(1, [rnn_input, state]), W) + b)\n",
    "\n",
    "\n",
    "## ALSO the part Adding rnn_cells to graph\n",
    "#state = init_state\n",
    "#rnn_outputs = []\n",
    "#for rnn_input in rnn_inputs:\n",
    "#    state = rnn_cell(rnn_input, state)\n",
    "#    rnn_outputs.append(state)\n",
    "#final_state = rnn_outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=4 color='green'> Logits and predictions are obtained in a different way. This is shown below.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Predictions, loss, training step FOR DYNAMIC RNN\n",
    "\"\"\"\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "#reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "logits = tf.matmul(rnn_outputs, W) + b\n",
    "predictions = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' color='green'>**Loss function and training step are also different**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For TF 1.\n",
    "losses = tf.reshape(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels= y_reshaped),\n",
    "                        [batch_size, num_steps])\n",
    "\n",
    "loss_by_timestep = tf.reduce_mean(losses, reduction_indices=0)\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss_by_timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Third: train the network</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Function to train the network**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to train the network\n",
    "\"\"\"\n",
    "\n",
    "def train_network(num_epochs, num_steps, state_size=4, verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #sess.run(tf.initialize_all_variables())\n",
    "        training_losses = []\n",
    "        rnn_outputs_save = []\n",
    "        print(\"Starting for idx...\")\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                rnn_out, training_loss_, training_state, _ = \\\n",
    "                    sess.run([rnn_outputs,loss_by_timestep,\n",
    "                              final_state,\n",
    "                              train_step],\n",
    "                                  feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_[-1]\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\n",
    "                              \"for last 100 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "                    rnn_outputs_save.append(rnn_out)\n",
    "\n",
    "    return training_losses, rnn_outputs_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Train the network**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting for idx...\n",
      "\n",
      "EPOCH 0\n",
      "Average loss at step 100 for last 100 steps: 0.657024950981\n",
      "Average loss at step 200 for last 100 steps: 0.589743927717\n",
      "Average loss at step 300 for last 100 steps: 0.542490180433\n",
      "Average loss at step 400 for last 100 steps: 0.504213443995\n",
      "Average loss at step 500 for last 100 steps: 0.497208533883\n",
      "Average loss at step 600 for last 100 steps: 0.492259939611\n",
      "Average loss at step 700 for last 100 steps: 0.485385486484\n",
      "Average loss at step 800 for last 100 steps: 0.480898011625\n",
      "Average loss at step 900 for last 100 steps: 0.479765984714\n"
     ]
    }
   ],
   "source": [
    "training_losses, rnn_outputs_save  = train_network(num_epochs,num_steps,state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**Plotting training losses**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xc447aa1e80>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>Finally: improve the model playing with hyperparameters num_steps state_size</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3' >**And try to understand your results!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...see next Notebook MSTC_RNN_3 ... for other RNN types..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
